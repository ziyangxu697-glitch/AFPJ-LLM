# AuditJudgLLM: A Domain-Specific Large Language Model and Benchmark for Evaluating Audit Judgment

## 🧾 项目概述

**项目类型：** 审计类型的数据蒸馏与大模型微调，并通过代码进行系统性评估与测试。  
**本仓库内容包括：**  
- 用于生成数据集的脚本；  
- 提取 `<think>` 过后答案的脚本；  
- 针对生成式、分类式、抽取式任务的数据集评估脚本；  
- 模型蒸馏与评估的完整实现框架。  

（此处可插入若干框架图，例如：模型蒸馏流程图、数据生成与评估架构图）

---

## ⚡ Quick Start

### 2.1 Obtaining Rationales

The following template should be provided as input to facilitate the **teacher model** to generate *rationales*.

```python
f"针对问题：'{question}'，生成完整且清晰的推理过程，确保最终答案与'{my_answer}'严格一致。推理过程需包含以下部分：\n"
f"---\n"
f"**1. 问题分析：**\n"
f"- 简要分析问题的核心内容，明确需要解决的关键点，包括任务要求的各个部分。\n"
f"\n"
f"**2. 推理步骤：**\n"
f"- **步骤 1**：识别问题的背景和任务要求，明确审计问题的表现形式。\n"
f"- **步骤 2**：根据表现形式，分析可能适用的法律法规，聚焦定性依据。\n"
f"- **步骤 3**：基于定性结果，确定处理处罚依据（若适用）。\n"
f"- **步骤 4**：验证推理结果与预期答案'{my_answer}'的一致性，调整格式或内容。\n"
f"\n"
f"**3. 最终答案（严格要求）：**\n"
f"- 必须严格等于预期答案：'{my_answer}'，包括标点和用词。\n"
f"- 直接输出答案，无额外解释。\n"
f"---\n"
f"⚠️ 注意：\n"
f"- 必须完整写出问题分析和至少 4 个推理步骤，不能跳跃。\n"
f"- 严格按照上述格式，确保逻辑详细且连贯。\n"

